{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, sys\n",
    "from fnmatch import fnmatch\n",
    "import re\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# LIWC vs. Vader.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import liwc\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LIWC - Linguistic Inquiry and Word Count \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, category_names = liwc.load_token_parser('../data/LIWC2007_English100131.dic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Vader Sentiment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Vader sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# analyzer.polarity_scores(text)\n",
    "# {'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.8393}\n",
    "\n",
    "# Function to create the sentiment dataframe from Submissions.\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Calc Sentiment Scores for each Submission.\n",
    "# Submissions\n",
    "# -------------------------------------------------\n",
    "def create_sentiment_submissions(df, user_base, save_path):\n",
    "    # new sentiment dataframe \n",
    "    columns = [\"subreddit\", \"unique_authors\", \"author\", \"selftext\", \"negative\", \"neutral\", \"positive\", \"compound\"]\n",
    "    sentiment_df = pd.DataFrame(columns=columns)\n",
    "    for index, row in df.iterrows():\n",
    "        score = analyzer.polarity_scores(row.selftext)\n",
    "        new_row = {\"subreddit\": row.subreddit, \"unique_authors\": user_base[row.subreddit], \"author\": row.author, \"selftext\": row.selftext, \"negative\": score['neg'], \"neutral\": score['neu'], \"positive\": score['pos'], \"compound\": score['compound']}\n",
    "        sentiment_df = pd.concat([sentiment_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        sentiment_df.to_csv(save_path)\n",
    "        # display(sentiment_df.sample())\n",
    "    return sentiment_df\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Calc Avg. Sentiment Scores for each Community.\n",
    "# Submissions\n",
    "# -------------------------------------------------\n",
    "def create_avg_sentiment_submissions(df, save_path):\n",
    "    list_of_subreddits = list(df.subreddit.unique())\n",
    "    # new AVG sentiment dataframe \n",
    "    columns = [\"subreddit\", \"unique_authors\", \"avg_negative\", \"avg_neutral\", \"avg_positive\", \"avg_compound\", \"sentiment\"]\n",
    "    avg_sentiment_df = pd.DataFrame(columns=columns)\n",
    "    for subreddit in list_of_subreddits:\n",
    "        # decide sentiment as positive, negative and neutral\n",
    "        sentiment = \"\"\n",
    "        # Average Sentiment Compound per Subreddit.\n",
    "        sentiment_score = df[df['subreddit'] == subreddit].compound.mean()\n",
    "        if sentiment_score >= 0.05 :\n",
    "            sentiment = \"Positive\"\n",
    "     \n",
    "        elif sentiment_score <= - 0.05 :\n",
    "            sentiment = \"Negative\"\n",
    "        else:\n",
    "            sentiment = \"Neutral\"\n",
    "        \n",
    "        new_row = {\"subreddit\": subreddit, \"unique_authors\": df[df['subreddit'] == subreddit].community_size.median(), \"avg_negative\": df[df['subreddit'] == subreddit].negative.mean(), \"avg_neutral\": df[df['subreddit'] == subreddit].neutral.mean(), \"avg_positive\": df[df['subreddit'] == subreddit].positive.mean(), \"avg_compound\": sentiment_score, \"sentiment\": sentiment}\n",
    "        avg_sentiment_df = pd.concat([avg_sentiment_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        avg_sentiment_df.to_json(save_path)\n",
    "        # display(avg_sentiment_df.sample())\n",
    "    return avg_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../submissions_preprocessed.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['subreddit'] == 'AntiPornVideos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_base = df.groupby(by=\"subreddit\")['author'].nunique()\n",
    "user_base['AdultSelfHarm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Vader sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# analyzer.polarity_scores(text)\n",
    "# {'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.8393}\n",
    "\n",
    "# Function to create the sentiment dataframe.\n",
    "def create_sentiment(df):\n",
    "    # new sentiment dataframe \n",
    "    columns = [\"subreddit\", \"community_size\", \"author\", \"selftext\", \"negative\", \"neutral\", \"positive\", \"compound\"]\n",
    "    sentiment_df = pd.DataFrame(columns=columns)\n",
    "    for index, row in df.iterrows():\n",
    "        score = analyzer.polarity_scores(row.selftext)\n",
    "        new_row = {\"subreddit\": row.subreddit, \"community_size\": user_base[row.subreddit], \"author\": row.author, \"selftext\": row.selftext, \"negative\": score['neg'], \"neutral\": score['neu'], \"positive\": score['pos'], \"compound\": score['neg']}\n",
    "        sentiment_df = pd.concat([sentiment_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        # display(sentiment_df.sample())\n",
    "    return sentiment_df\n",
    "    \n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../submissions_preprocessed.csv\", index_col=0)\n",
    "\n",
    "sentiment_df = create_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.to_json('../data/CommunityInfo.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('../data/CommunityInfo.json')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Average Information per Community\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/CommunityInfo.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../data/avg_community_sentiment.json\"\n",
    "avg_sentiment_df = create_avg_sentiment_submissions(df, save_path)\n",
    "\n",
    "display(avg_sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../data/avg_community_sentiment.json\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
