{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, sys\n",
    "from fnmatch import fnmatch\n",
    "import re\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import json\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Calculate Content Similarity based on the Average Submission for each Subreddit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs.\n",
    "# Max Sequence Length: \t384\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../../submissions_preprocessed.csv\", index_col=0)\n",
    "df = pd.read_json(\"../data/submissions_preprocessed.json\")\n",
    "save_path = \"submissions_embeddings.json\"\n",
    "df['word_embedding'] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_embedding(row):\n",
    "#     row.word_embedding = model.encode(row.selftext)\n",
    "\n",
    "def calc_embedding(text):\n",
    "    # convert_to_tensor False/True\n",
    "    return model.encode(text, convert_to_tensor=False)\n",
    "    # return model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "save_path = \"submissions_embeddings.json\"\n",
    "# df.apply(lambda x: calc_embedding(x), axis=1)\n",
    "df['word_embedding'] = df['selftext'].apply(calc_embedding) \n",
    "\n",
    "df.to_json(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"submissions_embeddings.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(\"submissions_embeddings.json\")\n",
    "unique_subreddits = list(df.subreddit.unique())\n",
    "columns = unique_subreddits\n",
    "content_similarity_matrix = pd.DataFrame(columns=columns)\n",
    "save_path = '../data/content_cosine_similarity_matrix.json'\n",
    "save_path_csv = '../data/content_cosine_similarity_matrix.csv'\n",
    "\n",
    "\n",
    "list_of_all_similarities = [] \n",
    "\n",
    "for subreddit in unique_subreddits:\n",
    "    similarities = []\n",
    "    for subreddit_to_compare in unique_subreddits:\n",
    "        print('\\n---\\nNow comparing: ', subreddit)\n",
    "        print('with: ' + subreddit_to_compare)\n",
    "        # ---------------------------------------------------------------------------------\n",
    "        # Special Apply Method to extract the string\n",
    "        #\n",
    "        print('Checkpoint: apply')\n",
    "        source_df = df[df['subreddit'] == subreddit]\n",
    "        target_df = df[df['subreddit'] == subreddit_to_compare]\n",
    "\n",
    "        # source_df['avg_embedding'] = source_df.word_embedding.apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "        # target_df['avg_embedding'] = target_df.word_embedding.apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "        source_df['avg_embedding'] = source_df.word_embedding\n",
    "        target_df['avg_embedding'] = target_df.word_embedding\n",
    "        # ---------------------------------------------------------------------------------\n",
    "        # String to Numpy Array\n",
    "        #\n",
    "        print('Checkpoint: to numpy')\n",
    "        source_temp = source_df['avg_embedding'].to_numpy()\n",
    "        target_temp = target_df['avg_embedding'].to_numpy()\n",
    "        # ---------------------------------------------------------------------------------\n",
    "        # Numpy Array to List\n",
    "        #\n",
    "        print('Checkpoint: to list')\n",
    "        source_temp = source_temp.tolist()\n",
    "        target_temp = target_temp.tolist()\n",
    "        # ---------------------------------------------------------------------------------\n",
    "        # Numpy Array to Tensor\n",
    "        #\n",
    "        print('Checkpoint: to tensor')\n",
    "        source_avg_embeddings = torch.Tensor(source_temp)\n",
    "        target_avg_embeddings = torch.Tensor(target_temp)\n",
    "        source_avg_embeddings = torch.mean(source_avg_embeddings, 1)\n",
    "        target_avg_embeddings = torch.mean(target_avg_embeddings, 1)\n",
    "        \n",
    "        # print('Checkpoint: to tensor')\n",
    "        # sub_tensors = torch.from_numpy(sub_temp)\n",
    "        # overlap_tensors = torch.from_numpy(overlap_temp)\n",
    "        \n",
    "        # Normalize\n",
    "        # print('Checkpoint: normalize')\n",
    "        # sub_tensors = torch.nn.functional.normalize(subreddit_avg_embeddings)\n",
    "        # overlap_tensors = torch.nn.functional.normalize(overlap_avg_embeddings)\n",
    "        \n",
    "        print('Checkpoint: to numpy')\n",
    "        a1 = source_avg_embeddings.numpy()\n",
    "        a2 = target_avg_embeddings.numpy()\n",
    "        \n",
    "        print(a1.shape)\n",
    "        # print(a2)\n",
    "        print(a2.shape)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if(len(a1) > len(a2)):\n",
    "            diff = len(a1) - len(a2)\n",
    "            print(\"Case 1\")\n",
    "            a2 = np.pad(a2, (0, len(a1) - len(a2)), 'constant')\n",
    "            # a2 = np.concatenate([a2, np.zeros(len(a1) - len(a2))])\n",
    "            # zeros = np.zeros((diff, 0))\n",
    "            # a2 = np.concatenate((a2[0], zeros))\n",
    "        else:\n",
    "            diff = len(a2) - len(a1)\n",
    "            print(\"Case 2\")\n",
    "            a1 = np.pad(a1, (0, len(a2) - len(a1)), 'constant')\n",
    "            # a1 = np.concatenate([a1, np.zeros(len(a2) - len(a1))])\n",
    "            # zeros = np.zeros((diff, 0))\n",
    "            # a1 = np.concatenate((a1[0], zeros))\n",
    "        \n",
    "        print(a1.shape)\n",
    "        print(a2.shape)\n",
    "        \n",
    "        # Back to Tensor\n",
    "        mean_embedding1 = torch.from_numpy(a1)\n",
    "        mean_embedding2 = torch.from_numpy(a2)\n",
    "        \n",
    "        # Compute cosine-similarities\n",
    "        cosine_scores = util.cos_sim(mean_embedding1, mean_embedding2)\n",
    "        # cosine_scores = cosine_similarity(mean_embedding1, mean_embedding2)\n",
    "        print(\"COSINE SCORE of the Average Embeddings:\")\n",
    "        print(cosine_scores[0].item())\n",
    "        similarities.append(cosine_scores[0].item())\n",
    "        print(\" Cosine Similarities\\n\", similarities)\n",
    "    list_of_all_similarities.append(similarities)\n",
    "cosine_matrix = pd.DataFrame(list_of_all_similarities, columns=unique_subreddits)\n",
    "cosine_matrix.index = unique_subreddits\n",
    "# cosine_matrix.to_json(save_path)\n",
    "cosine_matrix.to_csv(save_path_csv)\n",
    "print(\"finished\")\n",
    "display(cosine_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_csv = '../data/content_cosine_similarity_matrix.csv'\n",
    "save_path_json = '../data/content_cosine_similarity_matrix.json'\n",
    "df = pd.read_csv(save_path_csv, index_col=0)\n",
    "df.to_json(save_path_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
